{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danyl\\miniconda3\\envs\\myenv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), 'src')\n",
    "sys.path.append(os.path.abspath(src_dir))\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import model_2.utils.data_loader\n",
    "\n",
    "reload(model_2.utils.data_loader)\n",
    "\n",
    "from model_2.utils.data_loader import DataEncoderNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/dataset/ner_train.csv', sep=',')\n",
    "val_df = pd.read_csv('../data/dataset/ner_valid.csv', sep=',')\n",
    "test_df = pd.read_csv('../data/dataset/ner_test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "epochs = 1\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_sub_sentences = LabelEncoder()\n",
    "\n",
    "classes = [\"I\", \"O\", \"B\"]\n",
    "le_sub_sentences.fit(classes)\n",
    "\n",
    "train_df['status'] = le_sub_sentences.transform(train_df['status'])\n",
    "val_df['status'] = le_sub_sentences.transform(val_df['status'])\n",
    "test_df['status'] = le_sub_sentences.transform(test_df['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataEncoderNER(train_df[\"sub_sentence\"], train_df[\"status\"], tokenizer, max_len)\n",
    "val_dataset = DataEncoderNER(val_df[\"sub_sentence\"], val_df[\"status\"], tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out_linear = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        output_depart = self.out_linear(self.drop(pooled_output))\n",
    "\n",
    "        return output_depart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danyl\\AppData\\Local\\Temp\\ipykernel_7668\\2844148109.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./processed/model_2/NER_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification(len(le_sub_sentences.classes_))\n",
    "model.load_state_dict(torch.load(\"./processed/model_2/NER_model.pth\"))\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader_train, data_loader_valid, loss_fn, optimizer, device, current_epoch):\n",
    "    model = model.train()\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    correct_predictions_train = 0\n",
    "    correct_predictions_valid = 0\n",
    "\n",
    "    with tqdm(total=len(data_loader_train), desc=f\"Epoch {current_epoch}\", unit=\"batch\") as pbar:\n",
    "        for d in data_loader_train:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"status\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            correct_predictions_train += (outputs.argmax(1) == labels).sum().item()\n",
    "            losses_train.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    for d in data_loader_valid:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels= d[\"status\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        correct_predictions_valid += (outputs.argmax(1) == labels).sum().item()\n",
    "        losses_valid.append(loss.item())\n",
    "\n",
    "    train_acc = correct_predictions_train / len(data_loader_train.dataset)\n",
    "    train_loss = np.mean(losses_train)\n",
    "\n",
    "    valid_acc = correct_predictions_valid / len(data_loader_valid.dataset)\n",
    "    valid_loss = np.mean(losses_valid)\n",
    "\n",
    "    return {\"train_acc\": train_acc, \"train_loss\": train_loss, \"valid_acc\": valid_acc, \"valid_loss\": valid_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badf2ab278fa4893a5353c3eb74de88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2474 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_acc': 0.9990568325743081, 'train_loss': 0.003892944736765435, 'valid_acc': 0.9981136820925554, 'valid_loss': 0.005741413059782686}\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    results = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        epoch + 1\n",
    "    )\n",
    "    print(results)\n",
    "    history.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./processed/model_2/NER_model.pth\")\n",
    "results_df = pd.DataFrame(history)\n",
    "results_df.to_csv(\"./processed/model_2/NER_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END[O] Paris Marseille\n",
      "Status: O\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"END[O] Paris Marseille\"\n",
    "\n",
    "# Tokenisez la phrase\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérez les input_ids et attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "status = le_sub_sentences.inverse_transform(torch.max(outputs, 1).indices)\n",
    "\n",
    "print(sentence)\n",
    "print(f\"Status: {status[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference(model, sentence, tokenizer, label_encoder):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    status = label_encoder.inverse_transform(torch.max(outputs, 1).indices)\n",
    "    return status[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_names_from_sentence(sentence, model, tokenizer, label_encoder):\n",
    "    sentence = f\"END {sentence.lower()} END\"\n",
    "    doc_sentence = nlp(sentence)\n",
    "    tagged_sentence = \"\"\n",
    "\n",
    "    for idx, word in enumerate([token_word.text for token_word in doc_sentence]):\n",
    "        sub_sentence = doc_sentence[idx].text\n",
    "\n",
    "        if(idx + 1 > 1):\n",
    "            sub_sentence = f\"{sub_sentence}[{status}] \"\n",
    "            tagged_sentence += f\"{sub_sentence}\"\n",
    "\n",
    "        for word in doc_sentence[idx + 1 : idx + 7]:\n",
    "            word_to_add = word.text\n",
    "            if(word.text == \".\"):\n",
    "                word_to_add = \"None\"\n",
    "            sub_sentence = f\"{sub_sentence} {word_to_add}\"\n",
    "\n",
    "\n",
    "        status = make_inference(\n",
    "            model,\n",
    "            sub_sentence,\n",
    "            tokenizer,\n",
    "            label_encoder\n",
    "        )\n",
    "\n",
    "    individual_words = tagged_sentence.split()\n",
    "\n",
    "    names = []\n",
    "    for idx, word in enumerate(individual_words):\n",
    "        status = word[-3:]\n",
    "        trimmed_word = word[:-3]\n",
    "        if(status == \"[B]\"):\n",
    "            names.append(trimmed_word)\n",
    "\n",
    "        if(status == \"[I]\"):\n",
    "            if(trimmed_word == \".\"):\n",
    "                continue\n",
    "            names[len(names) - 1] = f\"{names[len(names) - 1]} {trimmed_word}\"\n",
    "\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m ville_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviodos abense de bas\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJe souhaite aller d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mville_1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m à \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mville_2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m en passant par Marseille. Tu sais ou je dois aller ?\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m----> 5\u001b[0m get_names_from_sentence(\n\u001b[0;32m      6\u001b[0m     sentence,\n\u001b[0;32m      7\u001b[0m     model,\n\u001b[0;32m      8\u001b[0m     tokenizer,\n\u001b[0;32m      9\u001b[0m     le_sub_sentences\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[77], line 20\u001b[0m, in \u001b[0;36mget_names_from_sentence\u001b[1;34m(sentence, model, tokenizer, label_encoder)\u001b[0m\n\u001b[0;32m     16\u001b[0m             word_to_add \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m         sub_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_to_add\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 20\u001b[0m     status \u001b[38;5;241m=\u001b[39m make_inference(\n\u001b[0;32m     21\u001b[0m         model,\n\u001b[0;32m     22\u001b[0m         sub_sentence,\n\u001b[0;32m     23\u001b[0m         tokenizer,\n\u001b[0;32m     24\u001b[0m         label_encoder\n\u001b[0;32m     25\u001b[0m     )\n\u001b[0;32m     27\u001b[0m individual_words \u001b[38;5;241m=\u001b[39m tagged_sentence\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m     29\u001b[0m names \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[76], line 9\u001b[0m, in \u001b[0;36mmake_inference\u001b[1;34m(model, sentence, tokenizer, label_encoder)\u001b[0m\n\u001b[0;32m      5\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m----> 9\u001b[0m status \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform labels back to original encoding.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m    Original encoding.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 153\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# inverse transform of empty array is empty array\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1381\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03marray([1, 1])\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1380\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[1;32m-> 1381\u001b[0m y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1382\u001b[0m     y,\n\u001b[0;32m   1383\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1384\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1385\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1386\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1387\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1388\u001b[0m )\n\u001b[0;32m   1390\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\myenv\\Lib\\site-packages\\torch\\_tensor.py:1149\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "ville_1 = \"abergement le petit\"\n",
    "ville_2 = \"viodos abense de bas\"\n",
    "sentence = f\"Je souhaite aller d'{ville_1} à {ville_2} en passant par Marseille. Tu sais ou je dois aller ?\" \n",
    "\n",
    "get_names_from_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    le_sub_sentences\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je souhaite aller de [[Paris]] à [[Marseille]] en passant par [[Lyon]]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
