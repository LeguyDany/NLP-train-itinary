{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "src_dir = os.path.join(os.getcwd(), 'src')\n",
    "sys.path.append(os.path.abspath(src_dir))\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "import src.data.dataset\n",
    "\n",
    "reload(src.data.dataset)\n",
    "\n",
    "from src.data.dataset import TrainDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_station_df = pd.read_csv('../data/liste-des-gares.csv', sep=';')\n",
    "\n",
    "departure_train_df = pd.read_csv('../data/dataset/departure_train.csv')\n",
    "departure_valid_df = pd.read_csv('../data/dataset/departure_valid.csv')\n",
    "\n",
    "arrival_train_df = pd.read_csv('../data/dataset/arrival_train.csv')\n",
    "arrival_valid_df = pd.read_csv('../data/dataset/arrival_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = departure_train_df\n",
    "valid_df = departure_valid_df\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 2\n",
    "learning_rate = 2e-5\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "bert_model = AutoModel.from_pretrained(\"dbmdz/bert-base-french-europeana-cased\")\n",
    "max_len = 128\n",
    "\n",
    "number_of_training_sentences_per_station = -1\n",
    "number_of_training_sentences_per_station_valid = -1\n",
    "number_of_train_stations = departure_train_df['departure'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5fb2b046e446fc9b5fa146a26a259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train_df = pd.DataFrame()\n",
    "new_valid_df = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=number_of_train_stations) as pbar:\n",
    "    for i in range(number_of_train_stations):\n",
    "        if number_of_training_sentences_per_station != -1 :\n",
    "            train_df_subset = train_df[train_df[\"departure_train_station_id\"] == i]\n",
    "            new_train_df = pd.concat([new_train_df, train_df_subset])\n",
    "        \n",
    "        if number_of_training_sentences_per_station_valid != -1:\n",
    "            valid_df_subset = valid_df[valid_df[\"departure_train_station_id\"] == i].head(number_of_training_sentences_per_station_valid)\n",
    "            new_valid_df = pd.concat([new_valid_df, valid_df_subset])\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "if number_of_training_sentences_per_station != -1:\n",
    "    train_df = new_train_df.reset_index(drop=True)\n",
    "if number_of_training_sentences_per_station_valid != -1:\n",
    "    valid_df = new_valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_train_stations = LabelEncoder()\n",
    "\n",
    "le_train_stations.fit(train_station_df[\"LIBELLE\"].unique())\n",
    "\n",
    "train_df['departure'] = le_train_stations.transform(train_df['departure'])\n",
    "train_df['arrival'] = le_train_stations.transform(train_df['arrival'])\n",
    "\n",
    "valid_df['departure'] = le_train_stations.transform(valid_df['departure'])\n",
    "valid_df['arrival'] = le_train_stations.transform(valid_df['arrival'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(\n",
    "    train_df[\"sentence\"], train_df[\"departure\"], train_df[\"arrival\"], tokenizer, max_len)\n",
    "val_dataset = TrainDataset(\n",
    "    valid_df[\"sentence\"], valid_df[\"departure\"], valid_df[\"arrival\"], tokenizer, max_len)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, n_depart, n_arrival):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out_depart = nn.Linear(self.bert.config.hidden_size, n_depart)\n",
    "        self.out_arrival = nn.Linear(self.bert.config.hidden_size, n_arrival)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        output_depart = self.out_depart(self.drop(pooled_output))\n",
    "        output_arrival = self.out_arrival(self.drop(pooled_output))\n",
    "\n",
    "        return output_depart, output_arrival\n",
    "\n",
    "    def freeze_arrival_layer(self):\n",
    "        for param in self.out_arrival.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_arrival_layer(self):\n",
    "        for param in self.out_arrival.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def freeze_depart_layer(self):\n",
    "        for param in self.out_depart.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_depart_layer(self):\n",
    "        for param in self.out_depart.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danyl\\AppData\\Local\\Temp\\ipykernel_9548\\1217334735.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./processed/departure_arrival_model2_trained.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification(len(le_train_stations.classes_), len(le_train_stations.classes_))\n",
    "model.load_state_dict(torch.load(\"./processed/departure_arrival_model2_trained.pth\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate\n",
    ")\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "def train_epoch(model, data_loader_train, data_loader_valid, loss_fn, optimizer, device, current_epoch):\n",
    "    model = model.train()\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    correct_predictions_train = 0\n",
    "    correct_predictions_valid = 0\n",
    "\n",
    "    with tqdm(total=len(data_loader_train), desc=f\"Epoch {current_epoch}\", unit=\"batch\") as pbar:\n",
    "        for d in data_loader_train:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels_depart = d[\"departure\"].to(device)\n",
    "            labels_arrival = d[\"arrival\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs_depart, outputs_arrival = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            loss_depart = loss_fn(outputs_depart, labels_depart)\n",
    "            loss_arrival = loss_fn(outputs_arrival, labels_arrival)\n",
    "            loss = loss_depart + loss_arrival\n",
    "\n",
    "            correct_predictions_train += (outputs_depart.argmax(1) == labels_depart).sum().item()\n",
    "            correct_predictions_train += (outputs_arrival.argmax(1) == labels_arrival).sum().item()\n",
    "\n",
    "            losses_train.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    for d in data_loader_valid:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels_depart = d[\"departure\"].to(device)\n",
    "        labels_arrival = d[\"arrival\"].to(device)\n",
    "\n",
    "        outputs_depart, outputs_arrival = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss_depart = loss_fn(outputs_depart, labels_depart)\n",
    "        loss_arrival = loss_fn(outputs_arrival, labels_arrival)\n",
    "        loss =  loss_depart + loss_arrival\n",
    "\n",
    "        correct_predictions_valid += (outputs_depart.argmax(1) == labels_depart).sum().item()\n",
    "        correct_predictions_valid += (outputs_arrival.argmax(1) == labels_arrival).sum().item()\n",
    "\n",
    "        losses_valid.append(loss.item())\n",
    "\n",
    "    train_acc = correct_predictions_train / (2 * len(data_loader_train.dataset))\n",
    "    train_loss = np.mean(losses_train)\n",
    "\n",
    "    valid_acc = correct_predictions_valid / (2 * len(data_loader_valid.dataset))\n",
    "    valid_loss = np.mean(losses_valid)\n",
    "\n",
    "    return {\"train_acc\": train_acc, \"train_loss\": train_loss, \"valid_acc\": valid_acc, \"valid_loss\": valid_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa9ae6a52774aa983a756409f023df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/17854 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Danyl\\miniconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_acc': 0.9957384480669451, 'train_loss': 0.2120521695508433, 'valid_acc': 0.9943845188721988, 'valid_loss': 0.07440525802924365}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf277465183432bbb36d756ab2291fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/17854 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_acc': 0.9968680160492006, 'train_loss': 0.07063993698016678, 'valid_acc': 0.9941841215627505, 'valid_loss': 0.05441564532915511}\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    results = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        epoch + 1\n",
    "    )\n",
    "    print(results)\n",
    "    history.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./processed/departure_arrival_model3_trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_history_df = pd.read_csv(\"./processed/model2_departure_arrival_training_history.csv\", index_col=0)\n",
    "results_df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_df = pd.concat([old_history_df, results_df], ignore_index=True)\n",
    "full_results_df.index += 1\n",
    "full_results_df.index.name = 'epoch'\n",
    "full_results_df.to_csv(\"./processed/model3_departure_arrival_training_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je souhaiterais aller de Anché-Voulon à Champagne-sur-Oise\n",
      "Depart: Anché-Voulon, Arrival: Champagne-sur-Oise\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"Je souhaiterais aller de {le_train_stations.classes_[67]} à {le_train_stations.classes_[596]}\"\n",
    "# Tokenisez la phrase\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérez les input_ids et attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "print(sentence)\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "depart = le_train_stations.inverse_transform(torch.max(outputs[0], 1).indices)\n",
    "arrival = le_train_stations.inverse_transform(torch.max(outputs[1], 1).indices)\n",
    "print(f\"Depart: {depart[0]}, Arrival: {arrival[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je souhaiterais aller de Anché-Voulon à Champagne-sur-Oise\n",
      "Depart: Anché-Voulon, Arrival: Champagne-sur-Oise\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"Je souhaiterais aller de {le_train_stations.classes_[67]} à {le_train_stations.classes_[596]}\"\n",
    "# Tokenisez la phrase\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérez les input_ids et attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "print(sentence)\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "depart = le_train_stations.inverse_transform(torch.max(outputs[0], 1).indices)\n",
    "arrival = le_train_stations.inverse_transform(torch.max(outputs[1], 1).indices)\n",
    "print(f\"Depart: {depart[0]}, Arrival: {arrival[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_acc</th>\n",
       "      <th>valid_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.633514</td>\n",
       "      <td>6.975745</td>\n",
       "      <td>0.983707</td>\n",
       "      <td>3.410824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991049</td>\n",
       "      <td>1.578010</td>\n",
       "      <td>0.994611</td>\n",
       "      <td>0.245290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995738</td>\n",
       "      <td>0.212052</td>\n",
       "      <td>0.994385</td>\n",
       "      <td>0.074405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.996868</td>\n",
       "      <td>0.070640</td>\n",
       "      <td>0.994184</td>\n",
       "      <td>0.054416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  train_loss  valid_acc  valid_loss\n",
       "0   0.633514    6.975745   0.983707    3.410824\n",
       "1   0.991049    1.578010   0.994611    0.245290\n",
       "0   0.995738    0.212052   0.994385    0.074405\n",
       "1   0.996868    0.070640   0.994184    0.054416"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je souhaiterais avoir un itineraire de Abancourt pour Abbeville passant par Acheux-Franleu.\n",
      "Depart: Abancourt, Arrival: Abbeville\n"
     ]
    }
   ],
   "source": [
    "sentence = f\"Je souhaiterais avoir un itineraire de Abancourt pour {le_train_stations.classes_[2]} passant par {le_train_stations.classes_[4]}.\"\n",
    "# Tokenisez la phrase\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "# Récupérez les input_ids et attention_mask\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "print(sentence)\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "depart = le_train_stations.inverse_transform(torch.max(outputs[0], 1).indices)\n",
    "arrival = le_train_stations.inverse_transform(torch.max(outputs[1], 1).indices)\n",
    "print(f\"Depart: {depart[0]}, Arrival: {arrival[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
